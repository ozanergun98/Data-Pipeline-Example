# Data-Pipeline-Example
For the pipeline to work, Docker Desktop should be downladed first. In the next step, docker-compose.yml file should be run with "docker-compose -f docker-compose.yml up -d" command in command prompt to build and start docker containers (It is useful to check if both zookeeper and kafka containers are started from Docker Desktop because Apache Kafka won't work without Zookeeper.). Then, logstash should be downloded and unzipped. Later, logstash.conf file should be put into logstash file. Later, csv_files directory should be created and csv files should be uploaded in there to read by through logstash (In FunctionsClass.py file the exact path of the csv_files should be written.). Next, kafka, pymongo, and elasticsearch libraries should be installed from the terminal. Lastly, when MainClass.py file is run data from selected csv file will be consumed in a kafka topic (when the file is run, it will update logstash.conf file according to the selected csv file in the csv_files directory and the kafka topic will be generated automatically.) and from there the data will be distributed in either MongoDB or Elasticsearch according to cpu value at the moment in real-time.
